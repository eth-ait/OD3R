<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="OD3R: Online Dense 3D Reconstruction of Humans and Scenes
from Monocular Videos">
  <meta name="keywords" content="OD3R">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OD3R: Online Dense 3D Reconstruction of Humans and Scenes
from Monocular Videos </title>


  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">OD3R</h1>
          <h1 class="title is-1 publication-title"> Online Dense 3D Reconstruction of Humans and Scenes from Monocular Videos</h1>

          <div class="is-size-4 publication-authors">
            <span class="author-block">CVPR 2025</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/ZetongZhangZ">Zetong Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://ait.ethz.ch/people/kamanuel">Manuel Kaufmann</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://lxxue.github.io">Lixin Xue</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://ait.ethz.ch/people/song">Jie Song</a><sup>1,2,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://oswaldm.github.io">Martin R. Oswald</a><sup>4</sup>,
            </span>
 
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ETH ZÃ¼rich,</span>
            <span class="author-block"><sup>3</sup>HKUST(GZ),</span>
            <span class="author-block"><sup>3</sup>HKUST,</span>
            <span class="author-block"><sup>4</sup>University of Amsterdam</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://eth-ait.github.io/OD3R/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://eth-ait.github.io/OD3R"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-archive"></i>
                  </span>
                  <span>SuppMat</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://eth-ait.github.io/OD3R"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/eth-ait/OD3R"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="./static/images/teaser.jpg"  class="center"/>
      <h2 class="subtitle has-text-centered">
        OD3R takes monocular RGB input videos of humans and jointly reconstructs a photorealistic dense Gaussian representation of
the scene and the moving human as well as camera poses, human poses, and human silhouettes within a SLAM setting.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Creating a photorealistic scene and human reconstruction from a single monocular in-the-wild video figures prominently in the perception of a human-centric 3D world. Recent neural rendering advances have enabled holistic human-scene reconstruction but require pre-calibrated camera and human poses, and days of training time. In this work, we introduce a novel unified framework that simultaneously performs camera tracking, human pose estimation and human-scene reconstruction in an online fashion. 3D Gaussian Splatting is utilized to learn Gaussian primitives for humans and scenes efficiently, and reconstruction-based camera tracking and human pose estimation modules are designed to enable holistic understanding and effective disentanglement of pose and appearance. Specifically, we design a human deformation module to reconstruct the details and enhance generalizability to out-of-distribution poses faithfully. Aiming to learn the spatial correlation between human and scene accurately, we introduce occlusion-aware human silhouette rendering and monocular geometric priors, which further improve reconstruction quality. Experiments on the EMDB and NeuMan datasets demonstrate superior or on-par performance with existing methods in camera tracking, human pose estimation, novel view synthesis and runtime. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video (coming)</h2>
        <div class="publication-video">
          <iframe width="560" height="315" src="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="section" id="method">
  <div class="container is-max-desktop content">
    <h2 class="title">Method Overview</h2>
    <img src="./static/images/pipeline.jpg"  height="250" class="center"/>
    <p>
      Given a monocular video featuring a human in the scene, we simultaneously track the camera and human poses for each frame while training 3D Gaussian primitives. Our holistic human-scene representation is designed to handle the garment deformations, shadows and scene occlusions. Camera and human pose optimization is achieved through dense matching for view synthesis and leveraging monocular geometric cues. Mapping is carried out within a small local keyframe window, and we propose multiple regularizations to enhance reconstruction quality from the sparse set of keyframes.
    </p>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{zhang2025od3r,
  author = {Zhang, Zetong and Kaufmann, Manuel and Xue, Lixin and Song, Jie and Oswald, Martin R},
  title = {{OD3R}: {O}nline {D}ense {3}D {R}econstruction of Humans and Scenes
from Monocular Videos},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2025}
}
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This webpage is built with the template from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>. We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
